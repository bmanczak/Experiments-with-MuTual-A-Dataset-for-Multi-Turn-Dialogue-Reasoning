#!/bin/bash

#SBATCH --partition=gpu_titanrtx_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=ExampleJob
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --time=04:00:00
#SBATCH --mem=32000M
#SBATCH --output=slurm_output_%A.out

module purge
module load 2019
module load Python/3.7.5-foss-2019b
module load CUDA/10.1.243
module load cuDNN/7.6.5.32-CUDA-10.1.243
module load NCCL/2.5.6-CUDA-10.1.243
module load Anaconda3/2018.12

# Your job starts in the directory where you call sbatch
cd $HOME/Experiments-with-MuTual-A-Dataset-for-Multi-Turn-Dialogue-Reasoning/
# Activate your environment
source activate ocn
pip list
# Run your code
python run.py --do_train --do_eval --do_lower_case --race_dir dream --model_dir bert-base-uncased --max_doc_len 400 --max_query_len 30 --max_option_len 16 --train_batch_size 24 --eval_batch_size 24 --learning_rate 1.5e-5 --num_train_epochs 5 --gradient_accumulation_steps 2 --output_dir output